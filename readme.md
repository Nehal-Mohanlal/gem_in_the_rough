Welcome to the future! 
Lol JK. But this is pretty cool 

LLMs can be used locally using Ollama. 
However, with some libraries installed with pip, we can use Python to interface with our LLM. 

Repo Description: 
A terminal-based program that allows you to continuously chat with the Gemma LLM. 

Getting Started. 
1. Clone the repo
2. run pip instal -r requirements.txt
3. Simply run the main.py file "python main.py"
4. A user input will appear in the terminal.
5. Type in your prompt and hit enter e.g. ("How many countries are in Asia")
6. A response will appear in the terminal along with a space for you to to continue your conversation.
7. Type "exit" to break the loop and exit.

I know it is very basic, but it is a start! 

: ) 
